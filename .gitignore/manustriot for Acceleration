Acceleration Method
Nowadays, the single particle electron cryo-microscopy (cryoEM) technique has been demonstrated outstanding capability of revealing the atomic-resolution structures of biological samples under nearly natural conditions. To provide an efficient also convenient platform to the biologists in the community of cryoEM, several programs (e.g., FREALIGN, RELION, cryoSPARC, etc) focusing on relative computing algorithms, from motion correction to three-dimensional (3D) reconstruction and classification, have been released with different features by some famous academic institutions or companies in the past few years. More recently, to chase a more powerful performance, a new program named THUNDER was developed by Tsinghua University, which highlights self-adaptive adjustment for estimation accuracy, the tolerance to bad particles and the defocus refinement.

As well known, in single particle cryoEM, a 3D reconstruction is calculated from a large number of individual images representing different views of the same molecule. The resolution of a 3D reconstruction is improved iteratively by refining the rotation, translation, defocus, and structural-state parameters of each particle. So, to reach a high resolution, especially when a particle exhibits little or no symmetry, a large number of images are often required to improve the signal-to-noise ratio (SNR) of the 3D reconstruction and to provide finer sampling in Fourier space. With the fast development of advanced electron microscope systems and large format digital image recording devices, it is now feasible to acquire very large single particle datasets. However, a large-size dataset consequently results in an extremely time-consuming data processing and thus the massive computational hardware, since the time required to process a dataset depends almost linearly on the number of particles in the dataset. As such, the high computational requirement for the cryoEM 3D reconstruction task has become a barrier even for larger centres, thus suggesting a necessary computing acceleration.

In, but not limited in, the THUNDER program, the core task of cryoEM 3D reconstruction is the iterative single-particle 3D alignment processing, which can be broadly divided into two steps: 1) The likelihood evaluation step at each alignment, where the distance of each input experimental particle image against a given 3D reference is calculated in order to evaluate the best matches, and the lowest distance denotes the highest likelihood; 2) The parameter estimation step, where the parameters for the particle image, i.e., rotation, translation, defocus and structural state for 3D classification, are estimated to reconstruct the 3D structure of the target macromolecule. Within the implementation of parameter estimation step in THUNDER, a specialized particle-filter algorithm was designed for the purpose of robustness of high-dimensional parameter estimation. Briefly, the particle-filter algorithm adopts statistical inference methods to find the complete solution of cryoEM parameter estimation, and was used in four subspaces of the rotation, translation, defocus and structural state (3D classification), respectively. For more detailed description of the algorithm, one can refer to our previous work [], where the remarkable performance of the particle-filter algorithm in resolution of 3D reconstructions has been demonstrated via rich experiments. However, there's no escaping the fact that the single-particle 3D alignment processing is still time-consuming, and occupies a large portion of computing time in the entire 3D reconstruction processing. Given the above, we take a deeper exploration into the alignment processing, and find out that the computational bottleneck mainly occurs in its former likelihood evaluation step. More specifically, given M experimental particle images and N projections of a 3D reference in a dataset, M*N times of the pair-wise image distance computations are totally required before the determination of all the best matches. One can imagine a huge computational cost when M and N become large. Therefore, it is worth doing the acceleration work in the likelihood evaluation step. 

In this work, we aim at shortening the time needed for the cryoEM 3D reconstruction in the THUNDER program, and mainly focus on alleviating the computational bottleneck in it, i.e., the likelihood evaluation step, via acceleration at the task and instruction set levels. 

Task-level parallelization
To perform an optimized implementation of THUNDER, we propose to leverage the combination of message passing interface (MPI) and open multi-processing (OpenMP) techniques for a more throughout parallelization at task level. First, the whole 3D reconstruction task is divided into a certain number of sub-tasks, i.e., processes, each of which takes in charge of the 3D reconstruction from a certain set of views. Then, each sub-task is further divided into smaller ones, i.e., threads, to perform the reconstruction at finer granularity. Therefore, the running mode for THUNDER can be interpreted as a recipe of multiple processes binding with multiple threads. Specifically, in each thread, four primarily computational procedures are conducted: 1) Projection generation: A small set of projections of current 3D reference model is calculated from pre-configured views and corrected by contrast transfer function (CTF); 2) Likelihood evaluation: The distance of each particle image against each involved projection is computed for the best matches selection. The lowest distance implies the highest likelihood. 3) Then parameter estimation: The parameters are estimated for the selected particles. 4) Finally reconstruction: These selected particles are aligned into current 3D reference model to form a reconstructed model at thread level. One should, however, note that the reconstructed model in each thread is an uncompleted model yet. So in THUNDER, an additionally merging processing is performed on all these thread-level models to generate a completed 3D reconstruction model.
 
To implement the above running mode physically, we arrange one physical computing node to take charge of one process, within which available CPU cores in the used computing node are then arranged to take charge of individual threads. In normal cases, the number of processes can be specified as the number of available computing nodes at hand, and the number of physical threads can be specified as the same number of CPU cores within one computing node. Under this running mode, the whole 3D reconstruction task can be processed in parallel at thread level finally, and thus reduce the computational time cost. With sufficient test, the cost of global communication inside THUNDER has been demonstrated to be considerably low (<5%). Given this, after the task level parallelization, our employed running mode is capable of showing good linear scalability, which means that with the number of compute nodes increases, the computational time cost of THUNDER will decrease linearly.

Instruction-level acceleration
For a clear description in what follows, here we present a brief introduction of the primary subroutine, i.e., computeImageDistance(), within the likelihood evaluation procedure, at the beginning of this section. Since it is the hottest computational spot within the whole 3D reconstruction task. Given a projection image after CTF correction, we have to compute the distance of every particle image against it, thus we can pick out the particles with the lowest distances, i.e., the highest likelihoods, for further parameter estimation. In computeImageDistance(), to evaluate the distance between two images, we first calculate the difference between each pair of pixels located at the same position in these two images. What should be noted is that, here, the difference value is in complex format, since the involved projection and particle are images in Fourier space and thus the pixels in them are in complex format. So formally, a difference between two pixels can be denoted as , where  and  are real numbers, and is an imaginary number. Next, we calculate  to represent the distance between a pair of pixels, and accumulate all the distances at pixel level together to form the final image distance between a project and a particle.

Beyond the task-level parallelization conducted in the previous section, three other optimization methods are further applied to THUNDER (mainly in, but not limited in,  the hotspot function computeImageDistance()) at instruction level. The first method is using as many as single-precision floating-point variables inside a thread to reduce both of the memory and computational consumption at the same time, instead of double-precision floating-point variables; The second method is inlining the implementation bodies of all complex arithmetic operations directly into the code of computeImageDistance() to avoid the call overhead of operator overloading; The third one is rewriting the computeImageDistance() with vectorization instructions to operate multiple data items in one instruction, which will efficiently improve the thread’s throughput during one clock cycle.

A.  Saving the cost of precision
In the previously released programs, including FREALIGN, RELION, cryoSPARC, the adopted variables during the whole 3D reconstruction processing are all declared in double-precision floating-point type, even through the projection images and the input particle image are saved in single-precision floating-point format. As well known, the single-precision floating-point format usually can represent numeric values at 10-6 precision level and occupies 32 bits in computer memory, whereas the double-precision floating-point format can represent numeric values at 10-15 prevision level but occupies 64 bits in memory. In the other words, comparing to the single-precision floating-point variables, the double-precision ones introduce not only a higher precision level but also a doubled memory and computational cost consequently. Taking this in mind, we decide to make an effort to replace some unnecessary double-precision floating-point variables with the single-precision ones for the purpose of cost saving, certainly under the premise of guaranteeing correct 3D reconstruction results.

As the aforementioned, for likelihood evaluation, we compute the image distance between every particle with a given projection after CTF correction, thus we can pick out the so-called best matched particles, i.e., particles with the lowest distances, for further parameter estimation. Given this, to pick out the best matched particles based on distances, the precision related to the distances needs to permit the representation of differences between each two of distances. We then take an investigation on a wide range of datesets, including xx,x,,x,, [], and especially investigate the numeric values of differences between each two of distances. We find that the smallest differences are at 10-3 (>= 10-6) precision level in fact, which means that the single-precision floating-point format is already enough to represents these distances related to particle images. Given the above, in THUNDER, we use the single-precision floating-point variables in the whole likelihood evaluation step to save memory and computational cost. Moreover, with the same consideration, we continuously use the single-precision floating-point variables in most calculations during the parameter estimation step, except only a few necessary calculations, including matrix rotation, rotation angles calculation, etc. Comparing to the implementation using double-precision floating-point variables thoroughly like in previously released programs, our used implementation is capable of saving a half cost of memory and, meanwhile, doubling the speed of 3D reconstruction as well.

B. Avoiding the cost of call for operator overloading 
During the implementation of distance calculation, i.e., computeImageDistance(), between a projection image after CTF correction and a particle image, the most frequently used operations are basic arithmetic operations between complex numbers, such as subtraction and multiplication and so on. The common implementation of these complex arithmetic operations is by means of overloading the operators already built in the used programming language. The main gain by using operator overloading is enabling one to implement these complex arithmetic operations within syntax reflecting common mathematical usage, which can therefore improve the clearness and readability of the code. However, operator overloading always introduces an extra performance overhead when the corresponding operation is called. Not only does the program pointer have to change, but in-use parameters have to be passed, and new variables allocated. Therefore, in fairness, the operator overloading is actually an trade-off between a clear and readable programming style and an acceptable performance overhead. In THUNDER, the subroutine computeImageDistance() usually needs to be run billions of times for a general-size dataset, which implies hundreds of billions of times runs of complex arithmetic operations. So, if we use the manner of operator overloading, a huge amount of call overhead can be thus expected. Given the above, we consider to inline the implementation bodies of all complex arithmetic operations directly into the code, instead of the calls of overloaded operators. Comparing to the implementation using operator overloading, our used implementation can further improve the 3D reconstruction’s performance relatively by 10%.

C.Vectorization
As mentioned before, the implementation of computeImageDistance() involves iterative complex arithmetic operations between pairs of pixels, so we consider to use the vectorization technique for acceleration. Vectorization is a kind of automatic parallelization technique widely supported by nowadays CPUs via single instruction, multiple data (SIMD) hardware. With it, a computer program can be converted from a scalar implementation, which processes a single pair of operands at a time, to a vector implementation, which processes one operation on multiple pairs of operands at once. 

Currently, there are a variety of SIMD vector instruction sets available to the end of vectorization, like MMX, AVX-2/512 and SSE1/2, etc. However, to our best knowledge, all these existing instruction sets provide vector instructions to only these basic types of numbers (e.g., integer, single-precision floating-point number, etc.), rather the complex numbers. So, to cope with this problem, we disassemble the complex arithmetic operations equally to basic arithmetic operations of their real and imaginary parts in single-precision floating-point format in advance, then we are enable to employ the vector operation instructions. Nowadays popular SIMD vector instructions are capable of processing data from 128 bits to 512 bits at once. So, in THUNDER, we implement two versions of vectorized computeImageDistance() based on the AVX-2 supporting 256-bit SIMD and the AVX-512 supporting 512-bit SIMD separately, and the user can choose a proper version depending on the SIMD hardware in use. For a brief but clear description, we present only the work relating to 256-bit vectorization in what follows, which can be easily extended to the 512-bit version. Within a 256-bit SIMD vector operation instruction, 8 pairs of single-precision floating-point type (i.e., 32 bits) operands can be calculated simultaneously. This indicates a significant performance improvement without doubt. 

An important preparation work before vector operations is loading the operands from memory to vector registers. To load single-precision floating-point values from memory to a 256-bit vector register, two intrinsic functions are available, i.e., _mm256_loadu_ps() and _mm256_load_ps(). But considering that the latter function _mm256_load_ps() only needs a half number of instruction cycles comparing to the former function, we decide to leverage _mm256_load_ps() for data loading in THUNDER. The first point one should note is that _mm256_load_ps() restricts to load single-precision floating-point values from a memory block starting with an address that must be aligned on a 256-bit boundary. That is, the address must be divisible by 256. To satisfy this, one has to use the functions with alignment restriction for memory allocation (e.g., aligned_alloc(size_t alignment, size_t size) for GNU systems, and _mm_alloc(size_t size, size_t aligment) for Intel). Besides the above, another underneath restriction one should pay attention is that the single-precision floating-point values to be loaded into one vector should be stored one after one in the same block of memory. To meet this restriction, we specifically arrange the storage of real and imaginary parts of complex numbers separately. As illustrated in Fig. X (缺), the real parts are stored in an array and the imaginary parts are stored in another one, instead of storing the real and imaginary parts alternatively in the same array as the traditional method does. After loading data from memory to vectors, we are able to perform vector operations consequently.

通过使用地址对齐，且数据地址连续的SIMD实现方式，THUNDER在步骤a优化的基础上，基于256位的向量化指令，性能获得了大约50%的提升，而使用512位的向量化指令，则获得了大约65%的性能提升。
